Transducer(
  (encoder): ConformerEncoder(
    (position_encoding): RelativePositionalEncoding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (subsampling): ConvolutionSubSampling(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (positional_encoding): RelativePositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (conformer_blocks): ModuleList(
      (0): ConformerEncoderLayer(
        (feedforward_one): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (attention): RelativeMultiHeadSelfAttentionModule(
          (pos_projection): Linear(in_features=256, out_features=256, bias=True)
          (w_key): Linear(in_features=256, out_features=256, bias=True)
          (w_query): Linear(in_features=256, out_features=256, bias=True)
          (w_value): Linear(in_features=256, out_features=256, bias=True)
          (projection): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (conv): ConvolutionModule(
          (pointwise_conv_one): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (glu): GLU(dim=1)
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (swish): SiLU()
          (pointwise_conv_two): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
        )
        (feedforward_two): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feedfoward_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (conv_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (final_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): ConformerEncoderLayer(
        (feedforward_one): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (attention): RelativeMultiHeadSelfAttentionModule(
          (pos_projection): Linear(in_features=256, out_features=256, bias=True)
          (w_key): Linear(in_features=256, out_features=256, bias=True)
          (w_query): Linear(in_features=256, out_features=256, bias=True)
          (w_value): Linear(in_features=256, out_features=256, bias=True)
          (projection): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (conv): ConvolutionModule(
          (pointwise_conv_one): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (glu): GLU(dim=1)
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (swish): SiLU()
          (pointwise_conv_two): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
        )
        (feedforward_two): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feedfoward_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (conv_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (final_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): ConformerEncoderLayer(
        (feedforward_one): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (attention): RelativeMultiHeadSelfAttentionModule(
          (pos_projection): Linear(in_features=256, out_features=256, bias=True)
          (w_key): Linear(in_features=256, out_features=256, bias=True)
          (w_query): Linear(in_features=256, out_features=256, bias=True)
          (w_value): Linear(in_features=256, out_features=256, bias=True)
          (projection): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (conv): ConvolutionModule(
          (pointwise_conv_one): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (glu): GLU(dim=1)
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (swish): SiLU()
          (pointwise_conv_two): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
        )
        (feedforward_two): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feedfoward_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (conv_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (final_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): ConformerEncoderLayer(
        (feedforward_one): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (attention): RelativeMultiHeadSelfAttentionModule(
          (pos_projection): Linear(in_features=256, out_features=256, bias=True)
          (w_key): Linear(in_features=256, out_features=256, bias=True)
          (w_query): Linear(in_features=256, out_features=256, bias=True)
          (w_value): Linear(in_features=256, out_features=256, bias=True)
          (projection): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (conv): ConvolutionModule(
          (pointwise_conv_one): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (glu): GLU(dim=1)
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (swish): SiLU()
          (pointwise_conv_two): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
        )
        (feedforward_two): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feedfoward_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (conv_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (final_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (4): ConformerEncoderLayer(
        (feedforward_one): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (attention): RelativeMultiHeadSelfAttentionModule(
          (pos_projection): Linear(in_features=256, out_features=256, bias=True)
          (w_key): Linear(in_features=256, out_features=256, bias=True)
          (w_query): Linear(in_features=256, out_features=256, bias=True)
          (w_value): Linear(in_features=256, out_features=256, bias=True)
          (projection): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (conv): ConvolutionModule(
          (pointwise_conv_one): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (glu): GLU(dim=1)
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (swish): SiLU()
          (pointwise_conv_two): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
        )
        (feedforward_two): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feedfoward_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (conv_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (final_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (5): ConformerEncoderLayer(
        (feedforward_one): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (attention): RelativeMultiHeadSelfAttentionModule(
          (pos_projection): Linear(in_features=256, out_features=256, bias=True)
          (w_key): Linear(in_features=256, out_features=256, bias=True)
          (w_query): Linear(in_features=256, out_features=256, bias=True)
          (w_value): Linear(in_features=256, out_features=256, bias=True)
          (projection): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (conv): ConvolutionModule(
          (pointwise_conv_one): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (glu): GLU(dim=1)
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (swish): SiLU()
          (pointwise_conv_two): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
        )
        (feedforward_two): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feedfoward_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (conv_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (final_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (6): ConformerEncoderLayer(
        (feedforward_one): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (attention): RelativeMultiHeadSelfAttentionModule(
          (pos_projection): Linear(in_features=256, out_features=256, bias=True)
          (w_key): Linear(in_features=256, out_features=256, bias=True)
          (w_query): Linear(in_features=256, out_features=256, bias=True)
          (w_value): Linear(in_features=256, out_features=256, bias=True)
          (projection): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (conv): ConvolutionModule(
          (pointwise_conv_one): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (glu): GLU(dim=1)
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (swish): SiLU()
          (pointwise_conv_two): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
        )
        (feedforward_two): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feedfoward_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (conv_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (final_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (7): ConformerEncoderLayer(
        (feedforward_one): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (attention): RelativeMultiHeadSelfAttentionModule(
          (pos_projection): Linear(in_features=256, out_features=256, bias=True)
          (w_key): Linear(in_features=256, out_features=256, bias=True)
          (w_query): Linear(in_features=256, out_features=256, bias=True)
          (w_value): Linear(in_features=256, out_features=256, bias=True)
          (projection): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (conv): ConvolutionModule(
          (pointwise_conv_one): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (glu): GLU(dim=1)
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (swish): SiLU()
          (pointwise_conv_two): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
        )
        (feedforward_two): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feedfoward_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (conv_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (final_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (8): ConformerEncoderLayer(
        (feedforward_one): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (attention): RelativeMultiHeadSelfAttentionModule(
          (pos_projection): Linear(in_features=256, out_features=256, bias=True)
          (w_key): Linear(in_features=256, out_features=256, bias=True)
          (w_query): Linear(in_features=256, out_features=256, bias=True)
          (w_value): Linear(in_features=256, out_features=256, bias=True)
          (projection): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (conv): ConvolutionModule(
          (pointwise_conv_one): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (glu): GLU(dim=1)
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (swish): SiLU()
          (pointwise_conv_two): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
        )
        (feedforward_two): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feedfoward_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (conv_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (final_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (9): ConformerEncoderLayer(
        (feedforward_one): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (attention): RelativeMultiHeadSelfAttentionModule(
          (pos_projection): Linear(in_features=256, out_features=256, bias=True)
          (w_key): Linear(in_features=256, out_features=256, bias=True)
          (w_query): Linear(in_features=256, out_features=256, bias=True)
          (w_value): Linear(in_features=256, out_features=256, bias=True)
          (projection): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (conv): ConvolutionModule(
          (pointwise_conv_one): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (glu): GLU(dim=1)
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (swish): SiLU()
          (pointwise_conv_two): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
        )
        (feedforward_two): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feedfoward_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (conv_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (final_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (10): ConformerEncoderLayer(
        (feedforward_one): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (attention): RelativeMultiHeadSelfAttentionModule(
          (pos_projection): Linear(in_features=256, out_features=256, bias=True)
          (w_key): Linear(in_features=256, out_features=256, bias=True)
          (w_query): Linear(in_features=256, out_features=256, bias=True)
          (w_value): Linear(in_features=256, out_features=256, bias=True)
          (projection): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (conv): ConvolutionModule(
          (pointwise_conv_one): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (glu): GLU(dim=1)
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (swish): SiLU()
          (pointwise_conv_two): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
        )
        (feedforward_two): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feedfoward_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (conv_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (final_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (11): ConformerEncoderLayer(
        (feedforward_one): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (attention): RelativeMultiHeadSelfAttentionModule(
          (pos_projection): Linear(in_features=256, out_features=256, bias=True)
          (w_key): Linear(in_features=256, out_features=256, bias=True)
          (w_query): Linear(in_features=256, out_features=256, bias=True)
          (w_value): Linear(in_features=256, out_features=256, bias=True)
          (projection): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (conv): ConvolutionModule(
          (pointwise_conv_one): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (glu): GLU(dim=1)
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (swish): SiLU()
          (pointwise_conv_two): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
        )
        (feedforward_two): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feedfoward_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (conv_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (final_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (predictor): Predictor(
    (embed): Embedding(5003, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (lstm): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activation): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5003, bias=True)
  )
  (attention_decoder): BiTransformerDecoder(
    (left_encoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5003, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (proj): Linear(in_features=256, out_features=5003, bias=True)
      (decoders): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attention): MultiHeadSelfAttentionModule(
            (w_key): Linear(in_features=256, out_features=256, bias=True)
            (w_query): Linear(in_features=256, out_features=256, bias=True)
            (w_value): Linear(in_features=256, out_features=256, bias=True)
            (projection): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attention): MultiHeadSelfAttentionModule(
            (w_key): Linear(in_features=256, out_features=256, bias=True)
            (w_query): Linear(in_features=256, out_features=256, bias=True)
            (w_value): Linear(in_features=256, out_features=256, bias=True)
            (projection): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feedforward): FeedForwardModule(
            (fc1): Linear(in_features=256, out_features=2048, bias=True)
            (swish): SiLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (fc2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attention): MultiHeadSelfAttentionModule(
            (w_key): Linear(in_features=256, out_features=256, bias=True)
            (w_query): Linear(in_features=256, out_features=256, bias=True)
            (w_value): Linear(in_features=256, out_features=256, bias=True)
            (projection): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attention): MultiHeadSelfAttentionModule(
            (w_key): Linear(in_features=256, out_features=256, bias=True)
            (w_query): Linear(in_features=256, out_features=256, bias=True)
            (w_value): Linear(in_features=256, out_features=256, bias=True)
            (projection): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feedforward): FeedForwardModule(
            (fc1): Linear(in_features=256, out_features=2048, bias=True)
            (swish): SiLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (fc2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attention): MultiHeadSelfAttentionModule(
            (w_key): Linear(in_features=256, out_features=256, bias=True)
            (w_query): Linear(in_features=256, out_features=256, bias=True)
            (w_value): Linear(in_features=256, out_features=256, bias=True)
            (projection): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attention): MultiHeadSelfAttentionModule(
            (w_key): Linear(in_features=256, out_features=256, bias=True)
            (w_query): Linear(in_features=256, out_features=256, bias=True)
            (w_value): Linear(in_features=256, out_features=256, bias=True)
            (projection): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feedforward): FeedForwardModule(
            (fc1): Linear(in_features=256, out_features=2048, bias=True)
            (swish): SiLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (fc2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (right_encoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5003, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (proj): Linear(in_features=256, out_features=5003, bias=True)
      (decoders): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attention): MultiHeadSelfAttentionModule(
            (w_key): Linear(in_features=256, out_features=256, bias=True)
            (w_query): Linear(in_features=256, out_features=256, bias=True)
            (w_value): Linear(in_features=256, out_features=256, bias=True)
            (projection): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attention): MultiHeadSelfAttentionModule(
            (w_key): Linear(in_features=256, out_features=256, bias=True)
            (w_query): Linear(in_features=256, out_features=256, bias=True)
            (w_value): Linear(in_features=256, out_features=256, bias=True)
            (projection): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feedforward): FeedForwardModule(
            (fc1): Linear(in_features=256, out_features=2048, bias=True)
            (swish): SiLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (fc2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attention): MultiHeadSelfAttentionModule(
            (w_key): Linear(in_features=256, out_features=256, bias=True)
            (w_query): Linear(in_features=256, out_features=256, bias=True)
            (w_value): Linear(in_features=256, out_features=256, bias=True)
            (projection): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attention): MultiHeadSelfAttentionModule(
            (w_key): Linear(in_features=256, out_features=256, bias=True)
            (w_query): Linear(in_features=256, out_features=256, bias=True)
            (w_value): Linear(in_features=256, out_features=256, bias=True)
            (projection): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feedforward): FeedForwardModule(
            (fc1): Linear(in_features=256, out_features=2048, bias=True)
            (swish): SiLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (fc2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attention): MultiHeadSelfAttentionModule(
            (w_key): Linear(in_features=256, out_features=256, bias=True)
            (w_query): Linear(in_features=256, out_features=256, bias=True)
            (w_value): Linear(in_features=256, out_features=256, bias=True)
            (projection): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attention): MultiHeadSelfAttentionModule(
            (w_key): Linear(in_features=256, out_features=256, bias=True)
            (w_query): Linear(in_features=256, out_features=256, bias=True)
            (w_value): Linear(in_features=256, out_features=256, bias=True)
            (projection): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feedforward): FeedForwardModule(
            (fc1): Linear(in_features=256, out_features=2048, bias=True)
            (swish): SiLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (fc2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (ctc_decoder): CTCDecoder(
    (proj): Linear(in_features=256, out_features=5003, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (criterion): CTCLoss()
  )
  (cmvn): GlobalCMVN()
  (simple_am_proj): Linear(in_features=256, out_features=5003, bias=True)
  (simple_lm_proj): Linear(in_features=256, out_features=5003, bias=True)
  (criterion_attn): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)Transducer(
  (encoder): ConformerEncoder(
    (position_encoding): RelativePositionalEncoding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (subsampling): ConvolutionSubSampling(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (positional_encoding): RelativePositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (conformer_blocks): ModuleList(
      (0): ConformerEncoderLayer(
        (feedforward_one): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (attention): RelativeMultiHeadSelfAttentionModule(
          (pos_projection): Linear(in_features=256, out_features=256, bias=True)
          (w_key): Linear(in_features=256, out_features=256, bias=True)
          (w_query): Linear(in_features=256, out_features=256, bias=True)
          (w_value): Linear(in_features=256, out_features=256, bias=True)
          (projection): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (conv): ConvolutionModule(
          (pointwise_conv_one): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (glu): GLU(dim=1)
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (swish): SiLU()
          (pointwise_conv_two): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
        )
        (feedforward_two): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feedfoward_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (conv_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (final_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): ConformerEncoderLayer(
        (feedforward_one): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (attention): RelativeMultiHeadSelfAttentionModule(
          (pos_projection): Linear(in_features=256, out_features=256, bias=True)
          (w_key): Linear(in_features=256, out_features=256, bias=True)
          (w_query): Linear(in_features=256, out_features=256, bias=True)
          (w_value): Linear(in_features=256, out_features=256, bias=True)
          (projection): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (conv): ConvolutionModule(
          (pointwise_conv_one): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (glu): GLU(dim=1)
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (swish): SiLU()
          (pointwise_conv_two): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
        )
        (feedforward_two): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feedfoward_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (conv_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (final_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): ConformerEncoderLayer(
        (feedforward_one): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (attention): RelativeMultiHeadSelfAttentionModule(
          (pos_projection): Linear(in_features=256, out_features=256, bias=True)
          (w_key): Linear(in_features=256, out_features=256, bias=True)
          (w_query): Linear(in_features=256, out_features=256, bias=True)
          (w_value): Linear(in_features=256, out_features=256, bias=True)
          (projection): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (conv): ConvolutionModule(
          (pointwise_conv_one): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (glu): GLU(dim=1)
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (swish): SiLU()
          (pointwise_conv_two): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
        )
        (feedforward_two): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feedfoward_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (conv_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (final_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): ConformerEncoderLayer(
        (feedforward_one): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (attention): RelativeMultiHeadSelfAttentionModule(
          (pos_projection): Linear(in_features=256, out_features=256, bias=True)
          (w_key): Linear(in_features=256, out_features=256, bias=True)
          (w_query): Linear(in_features=256, out_features=256, bias=True)
          (w_value): Linear(in_features=256, out_features=256, bias=True)
          (projection): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (conv): ConvolutionModule(
          (pointwise_conv_one): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (glu): GLU(dim=1)
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (swish): SiLU()
          (pointwise_conv_two): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
        )
        (feedforward_two): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feedfoward_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (conv_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (final_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (4): ConformerEncoderLayer(
        (feedforward_one): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (attention): RelativeMultiHeadSelfAttentionModule(
          (pos_projection): Linear(in_features=256, out_features=256, bias=True)
          (w_key): Linear(in_features=256, out_features=256, bias=True)
          (w_query): Linear(in_features=256, out_features=256, bias=True)
          (w_value): Linear(in_features=256, out_features=256, bias=True)
          (projection): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (conv): ConvolutionModule(
          (pointwise_conv_one): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (glu): GLU(dim=1)
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (swish): SiLU()
          (pointwise_conv_two): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
        )
        (feedforward_two): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feedfoward_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (conv_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (final_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (5): ConformerEncoderLayer(
        (feedforward_one): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (attention): RelativeMultiHeadSelfAttentionModule(
          (pos_projection): Linear(in_features=256, out_features=256, bias=True)
          (w_key): Linear(in_features=256, out_features=256, bias=True)
          (w_query): Linear(in_features=256, out_features=256, bias=True)
          (w_value): Linear(in_features=256, out_features=256, bias=True)
          (projection): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (conv): ConvolutionModule(
          (pointwise_conv_one): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (glu): GLU(dim=1)
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (swish): SiLU()
          (pointwise_conv_two): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
        )
        (feedforward_two): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feedfoward_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (conv_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (final_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (6): ConformerEncoderLayer(
        (feedforward_one): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (attention): RelativeMultiHeadSelfAttentionModule(
          (pos_projection): Linear(in_features=256, out_features=256, bias=True)
          (w_key): Linear(in_features=256, out_features=256, bias=True)
          (w_query): Linear(in_features=256, out_features=256, bias=True)
          (w_value): Linear(in_features=256, out_features=256, bias=True)
          (projection): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (conv): ConvolutionModule(
          (pointwise_conv_one): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (glu): GLU(dim=1)
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (swish): SiLU()
          (pointwise_conv_two): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
        )
        (feedforward_two): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feedfoward_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (conv_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (final_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (7): ConformerEncoderLayer(
        (feedforward_one): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (attention): RelativeMultiHeadSelfAttentionModule(
          (pos_projection): Linear(in_features=256, out_features=256, bias=True)
          (w_key): Linear(in_features=256, out_features=256, bias=True)
          (w_query): Linear(in_features=256, out_features=256, bias=True)
          (w_value): Linear(in_features=256, out_features=256, bias=True)
          (projection): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (conv): ConvolutionModule(
          (pointwise_conv_one): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (glu): GLU(dim=1)
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (swish): SiLU()
          (pointwise_conv_two): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
        )
        (feedforward_two): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feedfoward_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (conv_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (final_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (8): ConformerEncoderLayer(
        (feedforward_one): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (attention): RelativeMultiHeadSelfAttentionModule(
          (pos_projection): Linear(in_features=256, out_features=256, bias=True)
          (w_key): Linear(in_features=256, out_features=256, bias=True)
          (w_query): Linear(in_features=256, out_features=256, bias=True)
          (w_value): Linear(in_features=256, out_features=256, bias=True)
          (projection): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (conv): ConvolutionModule(
          (pointwise_conv_one): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (glu): GLU(dim=1)
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (swish): SiLU()
          (pointwise_conv_two): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
        )
        (feedforward_two): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feedfoward_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (conv_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (final_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (9): ConformerEncoderLayer(
        (feedforward_one): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (attention): RelativeMultiHeadSelfAttentionModule(
          (pos_projection): Linear(in_features=256, out_features=256, bias=True)
          (w_key): Linear(in_features=256, out_features=256, bias=True)
          (w_query): Linear(in_features=256, out_features=256, bias=True)
          (w_value): Linear(in_features=256, out_features=256, bias=True)
          (projection): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (conv): ConvolutionModule(
          (pointwise_conv_one): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (glu): GLU(dim=1)
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (swish): SiLU()
          (pointwise_conv_two): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
        )
        (feedforward_two): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feedfoward_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (conv_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (final_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (10): ConformerEncoderLayer(
        (feedforward_one): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (attention): RelativeMultiHeadSelfAttentionModule(
          (pos_projection): Linear(in_features=256, out_features=256, bias=True)
          (w_key): Linear(in_features=256, out_features=256, bias=True)
          (w_query): Linear(in_features=256, out_features=256, bias=True)
          (w_value): Linear(in_features=256, out_features=256, bias=True)
          (projection): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (conv): ConvolutionModule(
          (pointwise_conv_one): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (glu): GLU(dim=1)
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (swish): SiLU()
          (pointwise_conv_two): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
        )
        (feedforward_two): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feedfoward_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (conv_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (final_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (11): ConformerEncoderLayer(
        (feedforward_one): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (attention): RelativeMultiHeadSelfAttentionModule(
          (pos_projection): Linear(in_features=256, out_features=256, bias=True)
          (w_key): Linear(in_features=256, out_features=256, bias=True)
          (w_query): Linear(in_features=256, out_features=256, bias=True)
          (w_value): Linear(in_features=256, out_features=256, bias=True)
          (projection): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (conv): ConvolutionModule(
          (pointwise_conv_one): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (glu): GLU(dim=1)
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (swish): SiLU()
          (pointwise_conv_two): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
        )
        (feedforward_two): FeedForwardModule(
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (swish): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feedfoward_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (conv_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (final_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (predictor): Predictor(
    (embed): Embedding(5003, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (lstm): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activation): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5003, bias=True)
  )
  (attention_decoder): BiTransformerDecoder(
    (left_encoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5003, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (proj): Linear(in_features=256, out_features=5003, bias=True)
      (decoders): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attention): MultiHeadSelfAttentionModule(
            (w_key): Linear(in_features=256, out_features=256, bias=True)
            (w_query): Linear(in_features=256, out_features=256, bias=True)
            (w_value): Linear(in_features=256, out_features=256, bias=True)
            (projection): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attention): MultiHeadSelfAttentionModule(
            (w_key): Linear(in_features=256, out_features=256, bias=True)
            (w_query): Linear(in_features=256, out_features=256, bias=True)
            (w_value): Linear(in_features=256, out_features=256, bias=True)
            (projection): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feedforward): FeedForwardModule(
            (fc1): Linear(in_features=256, out_features=2048, bias=True)
            (swish): SiLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (fc2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attention): MultiHeadSelfAttentionModule(
            (w_key): Linear(in_features=256, out_features=256, bias=True)
            (w_query): Linear(in_features=256, out_features=256, bias=True)
            (w_value): Linear(in_features=256, out_features=256, bias=True)
            (projection): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attention): MultiHeadSelfAttentionModule(
            (w_key): Linear(in_features=256, out_features=256, bias=True)
            (w_query): Linear(in_features=256, out_features=256, bias=True)
            (w_value): Linear(in_features=256, out_features=256, bias=True)
            (projection): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feedforward): FeedForwardModule(
            (fc1): Linear(in_features=256, out_features=2048, bias=True)
            (swish): SiLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (fc2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attention): MultiHeadSelfAttentionModule(
            (w_key): Linear(in_features=256, out_features=256, bias=True)
            (w_query): Linear(in_features=256, out_features=256, bias=True)
            (w_value): Linear(in_features=256, out_features=256, bias=True)
            (projection): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attention): MultiHeadSelfAttentionModule(
            (w_key): Linear(in_features=256, out_features=256, bias=True)
            (w_query): Linear(in_features=256, out_features=256, bias=True)
            (w_value): Linear(in_features=256, out_features=256, bias=True)
            (projection): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feedforward): FeedForwardModule(
            (fc1): Linear(in_features=256, out_features=2048, bias=True)
            (swish): SiLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (fc2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (right_encoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5003, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (proj): Linear(in_features=256, out_features=5003, bias=True)
      (decoders): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attention): MultiHeadSelfAttentionModule(
            (w_key): Linear(in_features=256, out_features=256, bias=True)
            (w_query): Linear(in_features=256, out_features=256, bias=True)
            (w_value): Linear(in_features=256, out_features=256, bias=True)
            (projection): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attention): MultiHeadSelfAttentionModule(
            (w_key): Linear(in_features=256, out_features=256, bias=True)
            (w_query): Linear(in_features=256, out_features=256, bias=True)
            (w_value): Linear(in_features=256, out_features=256, bias=True)
            (projection): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feedforward): FeedForwardModule(
            (fc1): Linear(in_features=256, out_features=2048, bias=True)
            (swish): SiLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (fc2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attention): MultiHeadSelfAttentionModule(
            (w_key): Linear(in_features=256, out_features=256, bias=True)
            (w_query): Linear(in_features=256, out_features=256, bias=True)
            (w_value): Linear(in_features=256, out_features=256, bias=True)
            (projection): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attention): MultiHeadSelfAttentionModule(
            (w_key): Linear(in_features=256, out_features=256, bias=True)
            (w_query): Linear(in_features=256, out_features=256, bias=True)
            (w_value): Linear(in_features=256, out_features=256, bias=True)
            (projection): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feedforward): FeedForwardModule(
            (fc1): Linear(in_features=256, out_features=2048, bias=True)
            (swish): SiLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (fc2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attention): MultiHeadSelfAttentionModule(
            (w_key): Linear(in_features=256, out_features=256, bias=True)
            (w_query): Linear(in_features=256, out_features=256, bias=True)
            (w_value): Linear(in_features=256, out_features=256, bias=True)
            (projection): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attention): MultiHeadSelfAttentionModule(
            (w_key): Linear(in_features=256, out_features=256, bias=True)
            (w_query): Linear(in_features=256, out_features=256, bias=True)
            (w_value): Linear(in_features=256, out_features=256, bias=True)
            (projection): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feedforward): FeedForwardModule(
            (fc1): Linear(in_features=256, out_features=2048, bias=True)
            (swish): SiLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (fc2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (ctc_decoder): CTCDecoder(
    (proj): Linear(in_features=256, out_features=5003, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (criterion): CTCLoss()
  )
  (cmvn): GlobalCMVN()
  (simple_am_proj): Linear(in_features=256, out_features=5003, bias=True)
  (simple_lm_proj): Linear(in_features=256, out_features=5003, bias=True)
  (criterion_attn): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
Loaded 384 batches
Loaded 384 batches

    Name               Type                  Params 

 0  encoder            ConformerEncoder      33.5 M 
 1  predictor          Predictor              2.4 M 
 2  joint              TransducerJoint        2.8 M 
 3  attention_decoder  BiTransformerDecoder  14.6 M 
 4  ctc_decoder        CTCDecoder             1.3 M 
 5  cmvn               GlobalCMVN                 0 
 6  simple_am_proj     Linear                 1.3 M 
 7  simple_lm_proj     Linear                 1.3 M 
 8  criterion_attn     LabelSmoothingLoss         0 

Trainable params: 57.2 M                                                                                                                                                                             
Non-trainable params: 0                                                                                                                                                                              
Total params: 57.2 M                                                                                                                                                                                 
Total estimated model params size (MB): 228                                                                                                                                                          

Loaded 384 batches
Loaded 384 batches
Saving checkpoint to experiments/conformer-valid-test
Epoch 0/999                                         6/192 0:00:02  0:01:01 3.05it/s v_num: 8 train_loss_step: 1397.253 train_ctc_loss_step: 167.401 train_attn_loss_step: 342.771                  
                                                                                      train_rnnt_loss_step: 1772.129 train_batch_size_step: 6.0 lr_step: 0.0                                         
